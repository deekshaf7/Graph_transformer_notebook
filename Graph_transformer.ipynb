{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOz5iDqREYNZVx07b0HyY+A",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deekshaf7/Graph_transformer_notebook/blob/main/Graph_transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LcIpZiysfo11"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from mydataset import *\n",
        "from torch.utils.data import DataLoader\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.data import DataLoader as GeometricDataLoader\n",
        "from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "import torch.nn as nn\n",
        "from tqdm import tqdm\n",
        "import wandb\n",
        "from torch_geometric.nn import TransformerConv, global_mean_pool\n",
        "import torch.nn.functional as F\n",
        "\n",
        "PRINT = False\n",
        "\n",
        "# Define a Graph Transformer model for graph classification\n",
        "class GraphTransformerModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, num_layers, num_heads, dropout):\n",
        "        super(GraphTransformerModel, self).__init__()\n",
        "        # Initialize convolution layers\n",
        "        self.convs = nn.ModuleList()\n",
        "        self.convs.append(TransformerConv(input_dim, hidden_dim, heads=num_heads, dropout=dropout))\n",
        "        for _ in range(num_layers-1):\n",
        "            self.convs.append(TransformerConv(hidden_dim * num_heads, hidden_dim, heads=num_heads, dropout=dropout))\n",
        "            if PRINT:\n",
        "                print(\"conv layer size\", self.convs)\n",
        "\n",
        "        # Initialize fully connected layers\n",
        "        self.fc_layers = nn.ModuleList()\n",
        "        for _ in range(num_layers):\n",
        "            self.fc_layers.append(nn.Linear(hidden_dim * num_heads, hidden_dim * num_heads))\n",
        "\n",
        "        # Initialize output linear layers\n",
        "        self.fc_out_linear = nn.ModuleList()\n",
        "        for _ in range(num_heads):\n",
        "            self.fc_out_linear.append(nn.Linear(hidden_dim * num_heads, 3))  # Output layer to classify into 3 classes\n",
        "\n",
        "        # Print the size of each output linear layer\n",
        "        #for idx, linear_layer in enumerate(self.fc_out_linear):\n",
        "        #    print(f\"out_linear layer {idx} size:\", linear_layer.weight.size())\n",
        "\n",
        "        # Initialize softmax layers\n",
        "        self.fc_out_softmax = nn.ModuleList()\n",
        "        for _ in range(num_heads):\n",
        "            self.fc_out_softmax.append(nn.Softmax(dim=1))  # Softmax layer to convert logits to probabilities\n",
        "\n",
        "        # Initialize the fc_reduce layer (if needed)\n",
        "        self.fc_reduce = nn.Linear(hidden_dim * num_heads, 12)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
        "        if PRINT:\n",
        "            print(\"Input x shape:\", x.shape)\n",
        "            print(\"Input edge_index shape:\", edge_index.shape)\n",
        "            print(\"Input batch shape:\", batch.shape[0])\n",
        "            print(\"batch value: \", batch)\n",
        "\n",
        "        # Apply convolution layers\n",
        "        for i, conv in enumerate(self.convs):\n",
        "            x = conv(x, edge_index)\n",
        "            if PRINT:\n",
        "                print(f\"Shape after conv {i}:\", x.shape)\n",
        "            x = F.relu(x)  # Apply ReLU activation after each convolution layer\n",
        "            if PRINT:\n",
        "                print(f\"Shape after Conv ReLU {i}:\", x.shape)\n",
        "\n",
        "        #Pooling to get graph-level representation\n",
        "        x = global_mean_pool(x, batch)\n",
        "        if PRINT:\n",
        "            print(\"Shape after global mean pool:\", x.shape)\n",
        "\n",
        "        # Apply fully connected layers\n",
        "        for i, fc in enumerate(self.fc_layers):\n",
        "            x = fc(x)\n",
        "            if PRINT:\n",
        "                print(f\"Shape after fc {i}:\", x.shape)\n",
        "            x = F.relu(x)  # Apply ReLU activation after each fully connected layer\n",
        "            if PRINT:\n",
        "                print(f\"Shape after FC ReLU {i}:\", x.shape)\n",
        "\n",
        "        # Apply output layers\n",
        "        outputs = []\n",
        "        for linear, softmax in zip(self.fc_out_linear, self.fc_out_softmax):\n",
        "            logits = linear(x)\n",
        "            if PRINT:\n",
        "                print(f\"Shape after logits:\", logits.shape)\n",
        "            prob = softmax(logits)\n",
        "            if PRINT:\n",
        "                print(type(prob))\n",
        "            if PRINT:\n",
        "                print(f\"Shape after prob:\", prob.shape)\n",
        "\n",
        "            # Apply argmax to logits\n",
        "            #max_indices = torch.argmax(logits, dim=-1)\n",
        "            #if PRINT:\n",
        "            #    print(f\"Shape after argmax:\", max_indices.shape)\n",
        "\n",
        "            outputs.append(prob)\n",
        "            if PRINT:\n",
        "                print(f\"Shape after outputs:\", len(outputs))\n",
        "            if PRINT:\n",
        "                print(f\"Output Shape: \", len(outputs))\n",
        "\n",
        "        return outputs\n",
        "\n",
        "# Random seed for reproducibility\n",
        "seed = random.randint(0, 100000)\n",
        "print(f\"Seed: {seed}\")\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# HYPERPARAMS\n",
        "dataset = load_dataset(sys.argv[1])\n",
        "\n",
        "H = {\n",
        "        \"input_dim\" : 6 , # Define the input dimension based on the dataset\n",
        "        \"hidden_dim\" : 512 ,  # Dimension of the model\n",
        "        \"num_heads\" : 12 ,   # Number of attention heads\n",
        "        \"num_layers\" : 2 , # Number of layers\n",
        "        \"dropout\" : 0.1 , # Dropout rate\n",
        "}\n",
        "\n",
        "input_dim = 6  # Define the input dimension based on the dataset\n",
        "hidden_dim = 512  # Dimension of the model\n",
        "num_heads = 12\n",
        "num_layers = 4  # Number of encoder layers\n",
        "dropout = 0.1  # Dropout rate\n",
        "\n",
        "model = GraphTransformerModel(input_dim, hidden_dim, num_layers, num_heads, dropout).to(device)\n",
        "\n",
        "max_valid_metric = 0\n",
        "batch_size = 12\n",
        "learning_rate = 0.01\n",
        "num_epochs = 14\n",
        "size = 32\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "bar = tqdm(total=num_epochs)\n",
        "\n",
        "# Initialize optimizer and scheduler\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "scheduler = StepLR(optimizer, step_size=500, gamma=0.1)\n",
        "\n",
        "def wandb_init():\n",
        "    config={\n",
        "    \"learning_rate\": learning_rate,\n",
        "    \"dataset\": sys.argv[1],\n",
        "    \"epochs\": num_epochs,\n",
        "    \"batch_size\" : batch_size\n",
        "    }\n",
        "\n",
        "    config.update(H)\n",
        "    wandb.init(\n",
        "    # set the wandb project where this run will be logged\n",
        "        project=\"DETECTive_transformer\",\n",
        "        # track hyperparameters and run metadata\n",
        "        config=config\n",
        "    )\n",
        "\n",
        "def worker():\n",
        "\n",
        "    #wandb_init()\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    global batch_size, num_epochs, dataset\n",
        "\n",
        "    # Calculate the number of data points in each set\n",
        "    train_size = int(0.8 * len(dataset))\n",
        "    val_size = len(dataset) - train_size\n",
        "\n",
        "    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
        "    kwargs = {'num_workers': 8, 'pin_memory': True} if device == \"cuda\" else {}\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, collate_fn=collate_fn, batch_size=batch_size, drop_last=True, **kwargs)\n",
        "    val_loader = DataLoader(val_dataset, collate_fn=collate_fn, batch_size=batch_size, shuffle=False, drop_last=True, **kwargs)\n",
        "\n",
        "    max_valid_metric = 0\n",
        "    bar = tqdm(total=num_epochs)\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    scheduler = StepLR(optimizer, step_size=500, gamma=0.1)\n",
        "\n",
        "    if (len(sys.argv) >= 3):\n",
        "        tqdm.write(f\"Loading state dictionary from {sys.argv[2]}\")\n",
        "        state_dict = torch.load(sys.argv[2])\n",
        "        clean_state_dict = {}\n",
        "        for key, value in state_dict['model_sd'].items():\n",
        "            if key.startswith('module.'):\n",
        "                clean_state_dict[key[7:]] = value\n",
        "            else:\n",
        "                clean_state_dict[key] = value\n",
        "\n",
        "        model.load_state_dict(clean_state_dict)\n",
        "\n",
        "    #wandb.watch(model)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        epoch_losses = []\n",
        "        accuracies = []\n",
        "        valid_metric_train = []\n",
        "        for graphs_batch, batch_act_paths, batch_prop_paths, targets_batch, fault_type_batch in train_loader:\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            batch_loss = 0\n",
        "            if PRINT:\n",
        "                print(\"graphs:\", type(graphs_batch))\n",
        "                print(\"Graphs batch:\", graphs_batch)\n",
        "                print(\"fault_type:\", type(fault_type_batch))\n",
        "                print(\"fault type batch: \", fault_type_batch)\n",
        "                print(\"targets_type:\", type(targets_batch))\n",
        "                print(\"targets type batch: \", targets_batch)\n",
        "\n",
        "            fault_type_batch = fault_type_batch.float().view(-1, 1)  # Ensure fault_type is float and has shape [batch_size, 1]\n",
        "\n",
        "            # use these tensors in the model\n",
        "            pred_test_pattern = model(\n",
        "                graphs_batch\n",
        "            )\n",
        "\n",
        "            batch_loss = 0\n",
        "            if PRINT:\n",
        "                print(\"Fault type:\", fault_type_batch)\n",
        "                print(\"Targets:\", targets_batch)\n",
        "                print(\"Predict test pattern \", pred_test_pattern)\n",
        "\n",
        "            target_values = []\n",
        "            pred_values = []\n",
        "\n",
        "            for i in range(len(pred_test_pattern)):\n",
        "                if PRINT:\n",
        "                    print(f\"Predicted Test Pattern {i}: \\n\", pred_test_pattern[i])\n",
        "                    print(f\"Predicted Test Patterni Dim {i}: \\n\", torch.argmax(pred_test_pattern[i], dim=-1))\n",
        "                    print(f\"Target Pattern {i}: \\n\", targets_batch[i])\n",
        "                # Initialize variables to accumulate the errors\n",
        "                total_error = 0.0\n",
        "\n",
        "                target_length = len(targets_batch[i])\n",
        "                # Loop over the target batch values\n",
        "                for j in range(target_length):\n",
        "                    error = criterion(pred_test_pattern[i], torch.tensor(targets_batch[i][j]))\n",
        "                    #, dtype=torch.long))\n",
        "                    total_error += error\n",
        "\n",
        "                # Calculate the average error for the target batch values\n",
        "                avg_error = total_error / target_length\n",
        "\n",
        "                # Add the average error to the batch loss\n",
        "                batch_loss += avg_error\n",
        "\n",
        "                #batch_loss += criterion(pred_test_pattern[i], torch.tensor(targets_batch[i][0], dtype=torch.long))\n",
        "                accuracy = get_accuracy(torch.argmax(pred_test_pattern[i], dim=-1), targets_batch[i][0])\n",
        "                valid_metric_train.append(valid_tv(torch.argmax(pred_test_pattern[i], dim=-1), targets_batch[i]))\n",
        "                accuracies.append(accuracy)\n",
        "\n",
        "                #pred_values.append(torch.argmax(pred_test_pattern[i]).item())\n",
        "\n",
        "            epoch_losses.append(batch_loss.item())\n",
        "            # batch_loss += loss\n",
        "\n",
        "            batch_loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        avg_loss = np.average(epoch_losses)\n",
        "\n",
        "        '''\n",
        "        wandb.log({\n",
        "            \"train/Accuracy\" : np.average(accuracies),\n",
        "            \"train/Valid Metric\" : np.average(valid_metric_train),\n",
        "            \"train/Loss\" : avg_loss\n",
        "        })\n",
        "        '''\n",
        "        tqdm.write(f\"[Epoch {epoch}] Loss: {avg_loss : .2f} Train accuracy: {np.average(accuracies) : .2f} Valid Metric: {np.average(valid_metric_train):.2f}\")\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        ######################### ! VALIDATION ! ############################\n",
        "        if ((epoch + 1) % 10 == 0):\n",
        "            model.eval()\n",
        "            valid_metric = []\n",
        "            exact_val_accuracies = []\n",
        "\n",
        "            for graphs_batch, batch_act_paths, batch_prop_paths, targets_batch, fault_type_batch in val_loader:\n",
        "\n",
        "                with torch.no_grad():\n",
        "\n",
        "                    # use these tensors in the model\n",
        "                    pred_test_pattern = model(\n",
        "                        graphs_batch\n",
        "                        #fault_type_batch\n",
        "                    )\n",
        "                    #print(torch.round(pred_test_pattern[0]), '\\n',  targets_batch[0][0])\n",
        "                    for i in range(len(pred_test_pattern)):\n",
        "                        accuracy = get_accuracy(torch.argmax(pred_test_pattern[i], dim=-1), targets_batch[i][0])\n",
        "                        valid_metric_train.append(valid_tv(torch.argmax(pred_test_pattern[i], dim=-1), targets_batch[i]))\n",
        "                        exact_val_accuracies.append(accuracy)\n",
        "\n",
        "            valid_metric = np.average(valid_metric)\n",
        "            validation_accuracy = np.average(exact_val_accuracies)\n",
        "\n",
        "            if (valid_metric > max_valid_metric):\n",
        "                tqdm.write(f\"Found a new maximum, storing the model\")\n",
        "                max_valid_metric = valid_metric\n",
        "                # Save the models\n",
        "                torch.save({\n",
        "                    'model_sd' : model.state_dict(),\n",
        "                    'optim_sd' : optimizer.state_dict()\n",
        "                }, f'model.pt')\n",
        "            '''\n",
        "            wandb.log({\n",
        "                 \"val/Validation Accuracy\" : validation_accuracy,\n",
        "            #     \"val/Loose Validation Accuracy\" : loose_validation_accuracy,\n",
        "                 \"val/Valid Metric\" : valid_metric,\n",
        "             })\n",
        "            '''\n",
        "            tqdm.write(f'Validation accuracy = {validation_accuracy : .2f} Average valid tv = {valid_metric : .2f}')\n",
        "        bar.update()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    worker()"
      ]
    }
  ]
}